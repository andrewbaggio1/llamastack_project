services:
  ollama:
    build:
      context: ./ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - docker-net
    restart: unless-stopped

  llama-stack:
    image: llamastack/distribution-ollama:latest
    container_name: llama-stack
    ports:
      - "${LLAMA_STACK_PORT}:${LLAMA_STACK_PORT}"
    volumes:
      - ~/.llama:/root/.llama
    environment:
      INFERENCE_MODEL: "${INFERENCE_MODEL}"
      OLLAMA_URL: "http://ollama:11434"
    command: >
      --port ${LLAMA_STACK_PORT}
      --env INFERENCE_MODEL=${INFERENCE_MODEL}
      --env OLLAMA_URL=http://ollama:11434
    depends_on:
      - ollama
    networks:
      - docker-net
    restart: unless-stopped

  backend:
      build:
        context: ./backend
      container_name: backend
      volumes:
        - ./backend:/app
        - whisper_cache:/root/.cache/whisper
      working_dir: /app
      networks:
        - docker-net
      depends_on:
        - llama-stack
      environment:
        LLAMA_STACK_URL: "http://llama-stack:${LLAMA_STACK_PORT}"

      ports:
      - "5001:5001"                    # Expose Flask API
      command: python3 web_api.py 
      
      restart: unless-stopped
      develop:
        watch:
          - path: ./backend
            target: /app
            action: sync  # You can also use `rebuild` or `restart`


volumes:
  ollama_data:
  whisper_cache:

networks:
  docker-net:
    driver: bridge
    
